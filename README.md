# DoBot LLM Pick-and-Place System with YOLOv8 Detection

A complete robotic pick-and-place system combining **YOLOv8 object detection**, **Gemini LLM natural language control**, and **ROS2 MoveIt motion planning** for the DoBot Magician Lite robotic arm.

![ROS2](https://img.shields.io/badge/ROS2-Humble-blue)
![Python](https://img.shields.io/badge/Python-3.10+-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## âœ¨ Features

- **ğŸ¯ YOLOv8 Object Detection** - Detects 120 object classes (4 shapes Ã— 3 sizes Ã— 10 colors) with real-time 3D positioning
- **ğŸ¤– Gemini LLM Control** - Natural language command interpretation for intuitive robot control
- **ğŸ—£ï¸ Voice & Text Commands** - Speak or type commands to control the robot
- **ğŸ“ ROS2 MoveIt Integration** - Collision-free motion planning with RViz visualization
- **ğŸ”„ Camera Offset Compensation** - Automatic 50mm camera-to-suction offset correction
- **ğŸ“š Demonstration Learning** - Train policies through manual demonstrations
- **ğŸ  Automatic Home Return** - Robot returns to home position after tasks

## ğŸ“‹ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           User Interface                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Voice     â”‚     â”‚    Text     â”‚     â”‚   GUI       â”‚               â”‚
â”‚  â”‚  Commands   â”‚     â”‚  Commands   â”‚     â”‚  Controls   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Gemini LLM Commander                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Natural Language Processing â†’ JSON Robot Commands             â”‚    â”‚
â”‚  â”‚  "pick red cube" â†’ {"action": "pick_object", "color": "red"}   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLOv8 Vision   â”‚ â”‚  MoveIt Motion   â”‚ â”‚  Robot Control   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Detection  â”‚  â”‚ â”‚  â”‚ Planning   â”‚  â”‚ â”‚  â”‚ Execution  â”‚  â”‚
â”‚  â”‚ + Color    â”‚  â”‚ â”‚  â”‚ + IK       â”‚  â”‚ â”‚  â”‚ + Gripper  â”‚  â”‚
â”‚  â”‚ + Size     â”‚  â”‚ â”‚  â”‚ + Collisionâ”‚  â”‚ â”‚  â”‚ + Suction  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DoBot Magician Lite                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Ubuntu 22.04
- ROS2 Humble
- Python 3.10+
- USB Camera
- DoBot Magician Lite (optional for simulation)

### Installation

```bash
# 1. Install ROS2 Humble
sudo apt update
sudo apt install python3-pip python3-tk ros-humble-moveit ros-humble-rviz2 ros-humble-rosbridge-server

# 2. Install Python dependencies
pip install pymoveit2 google-generativeai opencv-python ultralytics SpeechRecognition pyaudio

# 3. Clone and build
git clone <this-repo-url>
cd dobot_rviz_ws
source /opt/ros/humble/setup.bash
colcon build
source install/setup.bash

# 4. Configure Gemini API (see Configuration section)
```

### Launch the System

```bash
# Terminal 1: Start RViz + MoveIt + Camera + YOLO
cd ~/dobot_rviz_ws
source install/setup.bash
ros2 launch launch/moveit_demo.launch.py

# Terminal 2: Start LLM Controller with GUI
python3 cloud_llm_controller.py
```

## ğŸ® Usage

### Text Commands

Type commands directly in the GUI:

```
move left 3 cm          # Move robot left by 3 centimeters
go up 2 cm              # Move robot up by 2 centimeters
pick red cube           # Pick up a red cube detected by YOLO
pick small blue sphere  # Pick specific size and color object
place at 0.2 0.0 0.1    # Place object at coordinates (x, y, z)
go home                 # Return to home position
go to ready             # Move to ready position
```

### Voice Commands

Click "Voice Command" button and speak:

- "move the robot left"
- "pick up the red block"
- "go back to home position"
- "place the object in front"

### Natural Language Examples

The Gemini LLM understands various phrasings:

| User Says | Robot Action |
|-----------|--------------|
| "Can you grab that red cube?" | Picks red cube |
| "Move a little to the left" | Moves 3cm left |
| "Put it down over there" | Places object |
| "Take me home" | Returns to home |

## ğŸ¤– Gemini LLM Commander

The system uses Google's Gemini AI to interpret natural language commands and convert them to robot actions.

### Command Flow

```
User Input â†’ Gemini API â†’ JSON Command â†’ Robot Execution
     â”‚            â”‚              â”‚              â”‚
     â–¼            â–¼              â–¼              â–¼
  "pick the    Process &    {"action":     Execute
   red cube"   Interpret    "pick_object", pick
                            "color":"red"} sequence
```

### Supported Actions

| Action | Parameters | Example |
|--------|------------|---------|
| `move_to_pose` | x, y, z (meters) | Relative movement |
| `pick_object` | size, color, type | Pick with YOLO detection |
| `place_object` | x, y, z | Place at coordinates |
| `home` | - | All joints to 0Â° |
| `ready` | - | Ready position (0, 0.5, -1.0, 0) rad |

### Movement Commands

Directional commands are interpreted as relative movements:

| Direction | Axis | Sign |
|-----------|------|------|
| Up/Down | Z | +/- |
| Left/Right | Y | +/- |
| Forward/Back | X | +/- |

### Coordinate Frame Reference

The robot uses the `base_link` frame:
- **+X** â†’ Forward (away from robot)
- **+Y** â†’ Left (from robot's perspective)
- **+Z** â†’ Up

### API Configuration

```python
# In cloud_llm_controller.py
api_key = "YOUR_GEMINI_API_KEY"  # Get from Google AI Studio
api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
```

### Error Handling

- **Rate Limiting (429)**: Automatic retry with exponential backoff (2^attempt seconds)
- **Invalid JSON**: Logged and command rejected
- **Network Errors**: Caught and reported to user

### GUI Features

The Gemini Desktop Commander GUI provides:
- **Text Entry**: Type natural language commands
- **Voice Button**: Click to speak commands
- **End Effector Selection**: Toggle between gripper and suction
- **Status Display**: Real-time command status
- **Response History**: View past commands and responses

## ğŸ¯ Object Detection

### YOLO Class Structure

Objects are classified as: `{size}_{color}_{shape}`

**Sizes**: small, medium, large (3 total)  
**Colors**: red, green, blue, yellow, orange, purple, pink, white, black, cyan (10 total)  
**Shapes**: cube, sphere, cylinder, pyramid (4 total)

### Detection Topics

| Topic | Type | Description |
|-------|------|-------------|
| `/yolo/detection_image` | Image | Annotated camera feed |
| `/yolo/detections` | String | Detection descriptions |
| `/yolo/target_pose` | PoseStamped | 3D object position |
| `/yolo/detection_markers` | MarkerArray | RViz visualization |

### Camera Calibration

The system uses homography calibration for pixel-to-robot coordinate transformation:

```python
# Calibration points (image pixels â†’ robot mm)
Image: [(152,29), (499,49), (481,388), (142,376)]
Robot: [(165,-100), (365,-100), (365,100), (165,100)]
```

## ğŸ“ Camera Offset Compensation

The camera is mounted 50mm ahead of the suction cup. The system automatically compensates:

```
Camera sees object at X=0.200m
         â†“
Offset applied: Suction moves to X=0.150m (0.200 - 0.050)
         â†“
Object centered under suction cup âœ“
```

### Physical Layout

```
wrist_pitch â†’ (+35mm Z) â†’ suction_cup â†’ (+25mm X) â†’ camera_link
                                                         â†“ (points down)
```

## ğŸ  Preset Positions

| Position | Joint Values (radians) | Description |
|----------|------------------------|-------------|
| Home | [0, 0, 0, 0] | All joints at zero |
| Ready | [0, 0.5, -1.0, 0] | Ready for picking |

## ğŸ› ï¸ Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| Robot not moving | Check RViz for "Solution found" message |
| YOLO not detecting | Verify camera feed: `ros2 topic echo /camera/image_raw` |
| Gemini not responding | Check API key and internet connection |
| Voice not working | Install: `pip install SpeechRecognition pyaudio` |
| Planning fails | Target may be out of reach or in collision |

### Verification Commands

```bash
# Check nodes
ros2 node list

# Check topics
ros2 topic list

# View detections
ros2 topic echo /yolo/detections

# Check transforms
ros2 run tf2_ros tf2_echo base_link camera_link

# Verify camera
ros2 topic hz /camera/image_raw
```

## ğŸ“š Detailed Documentation

For more in-depth information, see:

- [Installation Guide](INSTALL.md) - Detailed setup instructions
- [Usage Guide](USAGE_README.md) - Complete usage documentation
- [Integrated System Guide](INTEGRATED_SYSTEM_GUIDE.md) - Pick-place workflow
- [Planning Guide](PLANNING_GUIDE.md) - MoveIt motion planning details
- [Camera Calibration](calibrate_camera_position.md) - Camera setup
- [Demonstration Training](DEMONSTRATION_TRAINING_README.md) - Learning from demos
- [Camera Offset](CAMERA_OFFSET_SUMMARY.md) - Offset compensation details

## ğŸ“ Project Structure

```
dobot_rviz_ws/
â”œâ”€â”€ launch/
â”‚   â””â”€â”€ moveit_demo.launch.py      # Main launch file
â”œâ”€â”€ cloud_llm_controller.py        # Gemini LLM controller with GUI
â”œâ”€â”€ voice_text_controller.py       # Voice/text command handler
â”œâ”€â”€ yolo_detection_node.py         # YOLOv8 object detection
â”œâ”€â”€ camera_publisher.py            # Camera feed node
â”œâ”€â”€ integrated_pick_place_system.py # Complete pick-place system
â”œâ”€â”€ dobot_moveit_bridge.py         # MoveIt interface
â”œâ”€â”€ magician_moveit_config/        # MoveIt configuration
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ kinematics.yaml
â”‚       â”œâ”€â”€ ompl_planning.yaml
â”‚       â””â”€â”€ joint_limits.yaml
â”œâ”€â”€ urdf/                          # Robot description
â”œâ”€â”€ meshes/                        # 3D models
â””â”€â”€ rviz/                          # RViz configurations
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

## ğŸ“„ License

This project is licensed under the MIT License.

---

**Happy robot controlling! ğŸ¤–ğŸ™ï¸ğŸ“¹**
